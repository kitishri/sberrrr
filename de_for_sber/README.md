# Sber Analytics ETL Pipeline

## Описание

Этот проект реализует ETL-процесс с использованием **Apache Airflow**, **Redis**, **PostgreSQL** и других технологий для обработки и загрузки данных. Процесс включает в себя обработку данных, создание флагов для отслеживания статуса файлов, а также настройку логирования и мониторинга.

## Структура проекта

- **DAG**: Основной файл, где описаны все задачи и их зависимости.
- **Скрипты обработки данных**: Реализуют логику обработки данных — загрузка, трансформация и отправка в базу данных.
- **Логирование**: Логи сохраняются на хосте и отправляются в Elasticsearch для мониторинга.
- **Флаги статуса**: Создаются JSON-файлы для отслеживания статуса файлов на каждом шаге обработки данных.
- **Тестирование**: Тесты с использованием `pytest` и мокирования для проверки доступности Redis и правильности выполнения задач.

## Технологии

- **Apache Airflow**: Для автоматизации ETL-процесса.
- **Redis**: Для хранения флагов состояния файлов.
- **PostgreSQL**: Для хранения и загрузки данных.
- **Docker**: Для контейнеризации проекта.
- **Elasticsearch** и **Kibana**: Для мониторинга логов.
- **Prometheus** и **Grafana**: Для метрик и мониторинга производительности.

## Как запустить

1. **Настройка Docker**:
   - Создайте `.env` файл с переменными окружения, указывающими пути к данным и конфигурациям.
   - Используйте Docker Compose для поднятия контейнеров с Apache Airflow, PostgreSQL, Redis, Elasticsearch и Kibana.

2. **Запуск проекта**:
   - После настройки контейнеров запустите Airflow Web UI и перейдите к созданию и запуску DAG.

3. **Мониторинг**:
   - Логи, флаги и метрики доступны через Elasticsearch и Grafana.

## Пример работы

1. **Создание флагов**:
   Каждый этап обработки данных сопровождается созданием JSON-файлов с флагами, которые отслеживают статус файла на каждом этапе. Это позволяет контролировать состояние файлов и процесс их обработки.

2. **Логирование**:
   Все важные события, ошибки и информацию о статусе данных записываются в локальные лог-файлы, которые отправляются в Elasticsearch для дальнейшего анализа.

3. **Обработка данных**:
   Данные загружаются, проходят трансформацию и отправляются в базу данных PostgreSQL. При необходимости производится архивирование старых данных.

4. **Тестирование**:
   Для проверки работы с Redis и корректности выполнения ETL-процесса создаются тесты с моками.

## Архивные файлы

В архивных файлах хранится анализ и обработка больших наборов данных. Эти файлы служат основой для дальнейшей работы с данными и мониторинга.

## Контрибьюции

Если вы хотите улучшить проект или внести изменения, пожалуйста, создайте Pull Request с подробным описанием изменений.
